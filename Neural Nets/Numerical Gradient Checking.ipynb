{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.419011823876603e-10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    actiation function\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def forward(X,W1,W2):\n",
    "    \"\"\"\n",
    "    forward prop\n",
    "    \"\"\"\n",
    "    z2 = np.dot(X, W1)\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(a2, W2)\n",
    "    yHat = sigmoid(z3) \n",
    "    return [z2,a2,z3,yHat]\n",
    "\n",
    "def sigmoidPrime(z):\n",
    "    \"\"\"\n",
    "    grad activation function\n",
    "    \"\"\"\n",
    "    return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "\n",
    "def costFunction(X, y, W1, W2):\n",
    "    \"\"\"\n",
    "    cost/loss function\n",
    "    \"\"\"\n",
    "    fp = forward(X,W1,W2)\n",
    "    yHat = fp[-1]\n",
    "    J = 0.5*sum((y-yHat)**2)\n",
    "    return fp,J\n",
    "\n",
    "def costFunctionPrime(X, y, fp):\n",
    "    \"\"\"\n",
    "    backprop\n",
    "    \"\"\"\n",
    "    z2,a2,z3,yHat = fp\n",
    "    delta3 = np.multiply(-(y-yHat), sigmoidPrime(z3))\n",
    "    dJdW2 = np.dot(a2.T, delta3)\n",
    "    delta2 = np.dot(delta3, W2.T)*sigmoidPrime(z2)\n",
    "    dJdW1 = np.dot(X.T, delta2)  \n",
    "    return dJdW1, dJdW2\n",
    "\n",
    "# input, target, weights\n",
    "X = np.array([[0.3, 1. ],\n",
    "              [0.5 ,0.2],\n",
    "              [1.  ,0.4]]) \n",
    "Y = np.array([[0.75],\n",
    "              [0.82],\n",
    "              [0.93]])\n",
    "\"\"\"\n",
    "Note that these weights have already been trained over, which explains the small L2 norm. \n",
    "\"\"\"\n",
    "W1 = np.array([[ 1.62434536, -0.61175641 ,-0.52817175],\n",
    "               [-1.07296862 , 0.86540763 ,-2.3015387 ]]) \n",
    "W2 = np.array([[ 1.74481176],\n",
    "               [-0.7612069 ],\n",
    "               [ 0.3190391 ]])\n",
    "\n",
    "# forward training + loss\n",
    "fp,loss = costFunction(X, Y, W1, W2)\n",
    "\n",
    "# backward training (get gradients)\n",
    "dW1, dW2 = costFunctionPrime(X, Y, fp)\n",
    "\n",
    "# vectorize gradients (analytic)\n",
    "ANALYTICALgrad = np.concatenate((dW1.ravel(),dW2.ravel()))\n",
    "\n",
    "# vectorize parameters \n",
    "paramsInitial = np.concatenate((W1.ravel(),W2.ravel()))\n",
    "\n",
    "# array for numerical gradients\n",
    "NUMERICALgrad = np.zeros(paramsInitial.shape)\n",
    "\n",
    "# perturbation array +/- epsilon\n",
    "perturbs = np.zeros(paramsInitial.shape)\n",
    "\n",
    "# arrays for perturbed parameters\n",
    "paramsPlus = deepcopy(paramsInitial)\n",
    "paramsMinus= deepcopy(paramsInitial)\n",
    "\n",
    "# perturbation constant\n",
    "epsilon = 1E-4\n",
    "\n",
    "for weight in range(len(paramsInitial)):\n",
    "\n",
    "    \"\"\"\n",
    "    After vectorizing all weights, concatenate them into a single vector. Perturb by + epsilon each weight,  \n",
    "    then reshape all parameters back to their initial size. Do the same by pertube by - epsilon.\n",
    "    With only that single weight perturbed forward and backward, run both through NN and calculate the\n",
    "    two losses corresponding to the different pertubations. After obatining the\n",
    "    losses, calculate the numerical gradient for that weight. Reset the weight back to its original\n",
    "    value, then move on to the next weight and repeat. \n",
    "    \"\"\"\n",
    "\n",
    "    perturbs[weight] = epsilon\n",
    "        \n",
    "    ### Add small perturbation to each weight one at a time, then train the NN and get the loss   \n",
    "    paramsPlus[weight] += perturbs[weight]\n",
    "    ### reshape \n",
    "    w1_start = 0 \n",
    "    w1_end =  w1_start + W1.shape[0] * W1.shape[1]\n",
    "    W1 = np.reshape(paramsPlus[w1_start:w1_end], W1.shape)\n",
    "    w2_start = w1_end\n",
    "    w2_end = w2_start + W2.shape[0] * W2.shape[1]\n",
    "    W2 = np.reshape(paramsPlus[w2_start:w2_end], W2.shape)    \n",
    "    _ ,loss2 = costFunction(X, Y, W1, W2) # forward pass, get loss\n",
    "    paramsPlus[weight] -= perturbs[weight] # reset back to original\n",
    "    \n",
    "    ### Subtract small perturbation to each weight one at a time, then train the NN and get the loss \n",
    "    paramsMinus[weight] -= perturbs[weight]\n",
    "    ### reshape\n",
    "    w1_start = 0 \n",
    "    w1_end =  w1_start + W1.shape[0] * W1.shape[1]\n",
    "    W1 = np.reshape(paramsMinus[w1_start:w1_end], W1.shape)\n",
    "    w2_start = w1_end\n",
    "    w2_end = w2_start + W2.shape[0] * W2.shape[1]\n",
    "    W2 = np.reshape(paramsMinus[w2_start:w2_end], W2.shape)\n",
    "    _ ,loss1 = costFunction(X, Y, W1, W2) # forward pass, get loss    \n",
    "    paramsMinus[weight] += perturbs[weight] # reset back to original\n",
    "    \n",
    "    ### compute the numerical gradient\n",
    "    NUMERICALgrad[weight] = (loss2 - loss1)/(2*epsilon)\n",
    "\n",
    "# L2 Norm between analytic and numerical\n",
    "np.linalg.norm(ANALYTICALgrad-NUMERICALgrad)/np.linalg.norm(ANALYTICALgrad+NUMERICALgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
